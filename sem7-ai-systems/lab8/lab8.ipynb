{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Text generation with transformer\n",
    "\n",
    "* [Keras example](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)\n",
    "* [Tensorflow example](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "* [mlnuggets](https://www.machinelearningnuggets.com/transformer-decoder/)\n",
    "\n",
    "* [tensorflow GPU for cluster](https://www.tensorflow.org/install/pip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 20:08:57.765573: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736244537.785412 3622431 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736244537.791619 3622431 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-07 20:08:57.812198: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import setuptools.dist\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers, Model, Sequential, ops\n",
    "from keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20594\n"
     ]
    }
   ],
   "source": [
    "name_url = (\"Crime and Punishment\", 'https://www.gutenberg.org/files/2554/2554-0.txt')\n",
    "\n",
    "filepath = keras.utils.get_file(f'{name_url[0]}.txt', origin=name_url[1])\n",
    "text_f = ''\n",
    "with open(filepath, encoding='utf-8') as f:\n",
    "    text_f = f.read()[10000:] # skip preface +-\n",
    "\n",
    "text = text_f\n",
    "\n",
    "text = re.sub(r\"[\\\"\\`\\'\\’\\“\\”]\", r\"\", text_f)\n",
    "text = re.sub(r\"[\\(\\)]\", r\"\", text_f)\n",
    "text = re.sub(r\"[\\.\\!\\?]\", \"!\", text)\n",
    "text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "text_list = text.replace('\\n', ' ').split('!')\n",
    "text_list = list(map(lambda x: x.strip(), text_list))\n",
    "print( len(text_list) )\n",
    "\n",
    "text_list = list(filter(None, text_list))\n",
    "\n",
    "import random\n",
    "random.shuffle(text_list)\n",
    "\n",
    "length = len(text_list)\n",
    "text_train = text_list[:int(0.8*length)]\n",
    "text_valid = text_list[int(0.8*length):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736244541.868732 3622431 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31490 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:04:00.0, compute capability: 8.0\n",
      "I0000 00:00:1736244541.871781 3622431 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 32374 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:08:00.0, compute capability: 8.0\n",
      "I0000 00:00:1736244541.874649 3622431 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 32362 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:85:00.0, compute capability: 8.0\n",
      "I0000 00:00:1736244541.877101 3622431 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 32374 MB memory:  -> device: 3, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:89:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11697\n"
     ]
    }
   ],
   "source": [
    "words = sorted([(len(a:=line.split(\" \")), a, line) for line in text_list], reverse=1)\n",
    "MAX_TOKENS = MAX_LEN = max([len(line.split(\" \")) for line in text_list])\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LEN + 1,\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(text_list)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(len(vocab))\n",
    "\n",
    "word_from_id = tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=\"\", oov_token=\"[UNK]\",  invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 256\n",
    "\n",
    "def preprocess(text_l: list):\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        text = tf.expand_dims(text, -1)\n",
    "        tokenized_sentences = vectorize_layer(text)\n",
    "        x = tokenized_sentences[:, :-1]\n",
    "        y = tokenized_sentences[:, 1:]\n",
    "        return x, y\n",
    "    \n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices(text_l)\n",
    "            .shuffle(BUFFER_SIZE)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .map(preprocess_text)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "dataset_train = preprocess(text_train)\n",
    "dataset_valid = preprocess(text_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 102)\n",
      "(64, 102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 20:09:02.732329: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for X_train, y_train in dataset_train.take(1):\n",
    "    pass\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]  # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)  # (1, depth)\n",
    "    angle_rads = positions * angle_rates  # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x, value=x, key=x, return_attention_scores=True, use_causal_mask=True\n",
    "        )\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(dff, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(d_model),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.causal_self_attention.last_attn_scores\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(\n",
    "                d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 102)\n",
      "(64, 102, 512)\n",
      "(64, 102, 512)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb = PositionalEmbedding(len(vocab), 512)(X_train)\n",
    "sample_decoder = Decoder(\n",
    "    num_layers=4, d_model=512, num_heads=8, dff=2048, vocab_size=len(vocab)\n",
    ")\n",
    "\n",
    "# Print the shapes.\n",
    "print(X_train.shape)\n",
    "print(X_train_emb.shape)\n",
    "output = sample_decoder(x=X_train)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, *, num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dff=dff,\n",
    "            vocab_size=input_vocab_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(input_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "        # first argument.\n",
    "        x = inputs\n",
    "\n",
    "        x = self.decoder(x)  # (batch_size, target_len, d_model)\n",
    "\n",
    "        # Final linear layer output.\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Return the final output and the attention weights.\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "learning_rate = CustomSchedule(512)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "\n",
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    "    )\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers=4, d_model=512, num_heads=8, dff=2048, input_vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736244570.840616 3624437 service.cc:148] XLA service 0x7f4098013000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1736244570.840662 3624437 service.cc:156]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "I0000 00:00:1736244570.840669 3624437 service.cc:156]   StreamExecutor device (1): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "I0000 00:00:1736244570.840673 3624437 service.cc:156]   StreamExecutor device (2): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "I0000 00:00:1736244570.840679 3624437 service.cc:156]   StreamExecutor device (3): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2025-01-07 20:09:31.055100: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1736244571.610406 3624437 assert_op.cc:38] Ignoring Assert operator compile_loss/masked_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1736244572.212427 3624437 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-01-07 20:09:32.618214: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "2025-01-07 20:09:33.426017: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.030272: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6684', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.053144: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 284 bytes spill stores, 284 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.120918: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.183337: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.370401: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_97', 592 bytes spill stores, 620 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.435295: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_98', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.442181: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6684', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.465528: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 232 bytes spill stores, 232 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.571851: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_98', 184 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.626845: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.700345: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.703699: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.717813: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6684', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.723340: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_31_0', 160 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.732298: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_31', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.762132: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_31', 52 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.777927: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_31', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.809712: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_97', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.814934: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_98', 84 bytes spill stores, 100 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.827973: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_97', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.855545: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5_0', 776 bytes spill stores, 724 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.869730: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_98', 88 bytes spill stores, 88 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.913871: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6684', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.952735: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_97', 260 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.955852: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_97', 920 bytes spill stores, 888 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:34.977210: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 740 bytes spill stores, 572 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:35.122624: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6684', 756 bytes spill stores, 708 bytes spill loads\n",
      "\n",
      "2025-01-07 20:09:35.194411: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_98', 13888 bytes spill stores, 14296 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/204\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14:20\u001b[0m 22s/step - loss: 9.4069 - masked_accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736244581.367369 3624437 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m203/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 8.9801 - masked_accuracy: 0.0205"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1736244601.276260 3624440 assert_op.cc:38] Ignoring Assert operator compile_loss/masked_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-01-07 20:10:03.070196: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 300 bytes spill stores, 300 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:03.523881: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:03.544886: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 264 bytes spill stores, 264 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:03.886871: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5_0', 760 bytes spill stores, 448 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:03.959261: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87_0', 744 bytes spill stores, 572 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:04.010330: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_31_0', 160 bytes spill stores, 160 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 8.9776 - masked_accuracy: 0.0205"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1736244610.910983 3624440 assert_op.cc:38] Ignoring Assert operator compile_loss/masked_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1736244613.819643 3624438 assert_op.cc:38] Ignoring Assert operator compile_loss/masked_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-01-07 20:10:14.467420: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:14.834998: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:14.842756: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:14.924971: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 300 bytes spill stores, 300 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:15.037812: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:15.053699: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 264 bytes spill stores, 264 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:15.181225: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:15.232065: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5_0', 760 bytes spill stores, 448 bytes spill loads\n",
      "\n",
      "2025-01-07 20:10:15.281320: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29', 740 bytes spill stores, 572 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 175ms/step - loss: 8.9751 - masked_accuracy: 0.0206 - val_loss: 7.3582 - val_masked_accuracy: 0.0405\n",
      "Epoch 2/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 103ms/step - loss: 6.9115 - masked_accuracy: 0.0538 - val_loss: 6.2171 - val_masked_accuracy: 0.1102\n",
      "Epoch 3/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 103ms/step - loss: 5.9794 - masked_accuracy: 0.1213 - val_loss: 5.7610 - val_masked_accuracy: 0.1393\n",
      "Epoch 4/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 105ms/step - loss: 5.4300 - masked_accuracy: 0.1528 - val_loss: 5.5649 - val_masked_accuracy: 0.1518\n",
      "Epoch 5/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 105ms/step - loss: 5.0438 - masked_accuracy: 0.1773 - val_loss: 5.5241 - val_masked_accuracy: 0.1557\n",
      "Epoch 6/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 105ms/step - loss: 4.7135 - masked_accuracy: 0.2025 - val_loss: 5.5248 - val_masked_accuracy: 0.1554\n",
      "Epoch 7/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 105ms/step - loss: 4.3899 - masked_accuracy: 0.2312 - val_loss: 5.5639 - val_masked_accuracy: 0.1566\n",
      "Epoch 8/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - loss: 4.0559 - masked_accuracy: 0.2653 - val_loss: 5.6203 - val_masked_accuracy: 0.1526\n",
      "Epoch 9/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - loss: 3.6988 - masked_accuracy: 0.3069 - val_loss: 5.7434 - val_masked_accuracy: 0.1452\n",
      "Epoch 10/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - loss: 3.3430 - masked_accuracy: 0.3502 - val_loss: 5.9308 - val_masked_accuracy: 0.1354\n"
     ]
    }
   ],
   "source": [
    "transformer.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\n",
    "history = transformer.fit(dataset_train, epochs=10, validation_data=dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">48,001,536</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11697</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,000,561</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │    \u001b[38;5;34m48,001,536\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11697\u001b[0m)    │     \u001b[38;5;34m6,000,561\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">162,006,292</span> (618.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m162,006,292\u001b[0m (618.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,002,097</span> (206.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m54,002,097\u001b[0m (206.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">108,004,195</span> (412.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m108,004,195\u001b[0m (412.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        vocabulary,\n",
    "        transformer,\n",
    "        max_new_tokens,\n",
    "        temperature=0.0,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transformer = transformer\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "        sentence = self.tokenizer(sentence)\n",
    "        sentence = tf.expand_dims(sentence, axis=0)\n",
    "        encoder_input = sentence\n",
    "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "\n",
    "        print(f\"Generating {self.max_new_tokens} tokens\")\n",
    "        for i in tf.range(self.max_new_tokens):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer(encoder_input, training=False)\n",
    "\n",
    "            # Select the last token from the `seq_len` dimension.\n",
    "            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "            if self.temperature == 0.0:\n",
    "                # greedy sampling, output always the same\n",
    "                predicted_id = tf.argmax(predictions, axis=-1)\n",
    "            else:\n",
    "                predictions = predictions / self.temperature\n",
    "                predicted_id = tf.random.categorical(predictions[0], num_samples=1)\n",
    "\n",
    "            # Concatenate the `predicted_id` to the output which is given to the\n",
    "            # decoder as its input.\n",
    "            output_array = output_array.write(i + 1, predicted_id[0])\n",
    "            encoder_input = tf.experimental.numpy.append(encoder_input, predicted_id[0])\n",
    "            encoder_input = tf.expand_dims(encoder_input, axis=0)\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        # The output shape is `(1, tokens)`.\n",
    "        id_to_word = tf.keras.layers.StringLookup(\n",
    "            vocabulary=self.vocabulary, mask_token=\"\", oov_token=\"[UNK]\", invert=True\n",
    "        )\n",
    "\n",
    "        print(f\"Using temperature of {self.temperature}\")\n",
    "        text = id_to_word(output)\n",
    "        tokens = output\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop.\n",
    "        # So, recalculate them outside the loop.\n",
    "        self.transformer(output[:, :-1], training=False)\n",
    "        attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "        return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 50\n",
    "temperature = 0.8\n",
    "generator = Generator(\n",
    "    vectorize_layer, vocab, transformer, max_new_tokens, temperature, \n",
    ")\n",
    "def print_generation(sentence, generated_text):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Generation\":15s}: {generated_text}')\n",
    "    print(f'{\"Text\":15s}: {sentence} {\" \".join(list(map(lambda x: x.numpy().decode(\"utf-8\"), generated_text[0])))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50 tokens\n",
      "Using temperature of 0.8\n",
      "Input:         : Raskolnikov was a student who\n",
      "Generation     : [[b'' b'than' b'a' b'moment' b'of' b'lizaveta' b'who' b'he' b'had' b'on'\n",
      "  b'his' b'first' b'place' b'he' b'had' b'even' b'brought' b'him' b'a'\n",
      "  b'hundred' b'roubles' b'on' b'his' b'heart' b'even' b'more' b'even'\n",
      "  b'he' b'could' b'not' b'whether' b'he' b'blow' b'blow' b'he' b'was'\n",
      "  b'simply' b'a' b'new' b'fellow' b'at' b'once' b'into' b'there' b'on'\n",
      "  b'that' b'question' b'i' b'sum' b'of' b'everything']]\n",
      "Text           : Raskolnikov was a student who  than a moment of lizaveta who he had on his first place he had even brought him a hundred roubles on his heart even more even he could not whether he blow blow he was simply a new fellow at once into there on that question i sum of everything\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Raskolnikov was a student who\"\n",
    "generated_text, generated_tokens, attention_weights = generator(sentence)\n",
    "print_generation(sentence, generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
