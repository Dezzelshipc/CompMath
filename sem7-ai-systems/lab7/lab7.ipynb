{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Text generation with simple RNN\n",
    "\n",
    "* [Old tensorflow](https://www.tensorflow.org/text/tutorials/text_generation)\n",
    "* [Kaggle](https://www.kaggle.com/code/tirendazacademy/text-generation-with-tensorflow)\n",
    "* [mlnuggets](https://www.machinelearningnuggets.com/tensorflow-lstm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 20:20:42.377700: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736331642.397338 3919760 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736331642.403354 3919760 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers, Model, Sequential, ops\n",
    "from keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204802\n"
     ]
    }
   ],
   "source": [
    "name_url = (\"Crime and Punishment\", 'https://www.gutenberg.org/files/2554/2554-0.txt')\n",
    "\n",
    "filepath = keras.utils.get_file(f'{name_url[0]}.txt', origin=name_url[1])\n",
    "text_f = ''\n",
    "with open(filepath, encoding='utf-8') as f:\n",
    "    text_f = f.read()[10000:] # skip preface +-\n",
    "\n",
    "text = text_f\n",
    "\n",
    "text = re.sub(r\"[\\\"\\`\\'\\“\\”\\_]\", r\"\", text)\n",
    "text = re.sub(r\"[\\(\\)]\", r\"\", text)\n",
    "# text = re.sub(r\"[\\.\\!\\?]\", \"!\", text)\n",
    "text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "text_list = text.split(' ')\n",
    "text_list = list(map(lambda x: x.strip(), text_list))\n",
    "print( len(text_list) )\n",
    "\n",
    "text_list = list(filter(None, text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736331651.025838 3919760 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17275 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:04:00.0, compute capability: 8.0\n",
      "I0000 00:00:1736331651.028567 3919760 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 17327 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:08:00.0, compute capability: 8.0\n",
      "I0000 00:00:1736331651.031134 3919760 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 18241 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:85:00.0, compute capability: 8.0\n",
      "I0000 00:00:1736331651.033314 3919760 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 15517 MB memory:  -> device: 3, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:89:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10879\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(text)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(len(vocab))\n",
    "\n",
    "word_from_id = tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=\"\", oov_token=\"[UNK]\",  invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 128\n",
    "MAX_LEN = 100\n",
    "\n",
    "def preprocess(text_l: list, length=MAX_LEN, seed=None, batch_size=BATCH_SIZE):\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        return text[:, :-1], text[:, 1:]\n",
    "    \n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices(text_l)\n",
    "            .window(length + 1, shift=1, drop_remainder=True)\n",
    "            .flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "            .shuffle(BUFFER_SIZE, seed=seed)\n",
    "            .batch(batch_size)\n",
    "            .map(preprocess_text)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "enc_train = vectorize_layer(\" \".join(text_list[:len(text_list)*3//4]))\n",
    "enc_valid = vectorize_layer(\" \".join(text_list[len(text_list)*3//4:]))\n",
    "\n",
    "dataset_train = preprocess(enc_train)\n",
    "dataset_valid = preprocess(enc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(vocab), output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(len(vocab), activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 100)\n",
      "(16, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_train, y_train in dataset_train.take(1):\n",
    "    pass\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='nadam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736331660.096509 3920029 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9591/Unknown \u001b[1m155s\u001b[0m 16ms/step - accuracy: 0.1334 - loss: 5.3167"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derzhapolskii.yv/tf/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9594/9594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 20ms/step - accuracy: 0.1335 - loss: 5.3165 - val_accuracy: 0.0745 - val_loss: 6.8279\n",
      "Epoch 2/5\n",
      "\u001b[1m9594/9594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 19ms/step - accuracy: 0.2131 - loss: 4.5251 - val_accuracy: 0.0840 - val_loss: 6.9625\n",
      "Epoch 3/5\n",
      "\u001b[1m9594/9594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 19ms/step - accuracy: 0.2447 - loss: 4.1461 - val_accuracy: 0.0877 - val_loss: 7.0460\n",
      "Epoch 4/5\n",
      "\u001b[1m9594/9594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 20ms/step - accuracy: 0.2731 - loss: 3.8370 - val_accuracy: 0.0882 - val_loss: 7.1615\n",
      "Epoch 5/5\n",
      "\u001b[1m9594/9594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 19ms/step - accuracy: 0.3000 - loss: 3.5954 - val_accuracy: 0.0887 - val_loss: 7.2761\n"
     ]
    }
   ],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"checkpoint{epoch}.weights.h5\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train, \n",
    "    epochs=5, \n",
    "    validation_data=dataset_valid,\n",
    "    callbacks=[checkpoint_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(text, temperature=1):\n",
    "    y_proba = model.predict(vectorize_layer([text]), verbose=0)[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    w_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return word_from_id([w_id])\n",
    "\n",
    "def extend_text(text, n_words=20, temperature=1):\n",
    "    for _ in range(n_words):\n",
    "        w = next_word(text, temperature)\n",
    "        text += \" \"\n",
    "        text += w\n",
    "        # print(w)\n",
    "    return text.numpy()[0].decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Raskolnikov was a murderer i wanted it her proved myself well i forced for myself one thing clearly so i thought of'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extend_text(\"Raskolnikov was\", n_words=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
